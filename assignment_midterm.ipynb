{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Midterm test and practice session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Questions.\n",
    "Please, answer the following questions briefly. Two or three sentences with main idea would be enough.\n",
    "\n",
    "Do not use external resourses in this part, please. Answer with you own words. If you forgot something, don't worry, we will discuss it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0. \n",
    "Please, formulate the supervised learning problem statement.\n",
    "\n",
    "We have made a number of observations, taking record of some feature values which we think are important in making some predicting a target. Given new ovservations features, we want to be able to predict the target value by learning the relationship from our previous observations and their target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.\n",
    "\n",
    "What are regression and classification problems. Whatâ€™s the difference?\n",
    "\n",
    "Regression problems are problems in which our target value comes from a continuous range of values. Classification problems are problems in which our target value comes from a discrete set of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.\n",
    "Write down the linear model for regression problem in matrix notation. What is Mean Squared Error (MSE) loss function? How can it be expressed?\n",
    "\n",
    "Given a feature matrix X, and its target value, a vector Y, we are trying to find the best weights, a vector w, which when multiplied by X gives a prediction, y_hat which is close or equal to our original target Y.\n",
    "\n",
    "That is, y_hat = X * w.\n",
    "\n",
    "The Mean Squared Error is a loss function which measures how far our predictions are from the actual target values. It is expressed as:\n",
    "\n",
    "MSE = (1/n) * sum((y_hat - Y)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.\n",
    "What is the gradient of a function? How is it being used in optimization?\n",
    "\n",
    "The gradient of a function is an expression which yields the rate of change of the function's output wrt its input.\n",
    "\n",
    "During optimization, we are trying to find a minimum or maximum point of a function's output. We know that at these extreme points the gradient's value is zero and that the closer we are to these points, the smaller the gradient's value is. We can use this information, given a starting point, to incrementally move towards the extreme point by following the direction of the negative gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.\n",
    "Write down gradient descent step for linear model and MSE for one-dimensional case.\n",
    "\n",
    "Say we make a prediction y_hat = w * x, and the actual label is y. The gradient of the MSE loss function wrt w is:\n",
    "\n",
    "dMSE/dw = (2/n) * sum((y_hat - y) * x)\n",
    "\n",
    "The gradient descent step is then:\n",
    "\n",
    "w = w - alpha * dMSE/dw\n",
    "\n",
    "where alpha is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.\n",
    "What is validation? Cross validation?\n",
    "\n",
    "Validation is the phase of building our model in which we test our model's performance on inputs it has never seen before / was not trained on.\n",
    "\n",
    "Cross validation is used to test how the model performs on different subsets of the data. It ensures that the model does not overfit to its training data and allows it to better generalize. It can also be used to find the best hyperparameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.\n",
    "What is regularization? How does L1 regularization differ from L2 for linear models?\n",
    "\n",
    "Regularization is a method used to prevent overfitting. It does this by adding a penalty term to the loss function which depends on the weights of the model, with bigger weights causing a higher penalty.\n",
    "\n",
    "L1 regularization's penalty term is the sum of the absolute values of the weights. L2 regularization uses the sum of the squares of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.\n",
    "What are precision and recall metrics?\n",
    "\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "Precision gives us an idea of how many of the positive predictions were actually correct.\n",
    "\n",
    "Recall gives us an idea of how many of the actual positive values were predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.\n",
    "What is bagging? What is the main idea beneath it?\n",
    "\n",
    "In bagging, we pick random subsets of the data to train different models. When we want to make a prediction, we have all these models predict, then either take the average (in regression) or the mode (in classification) of the predictions as our final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.\n",
    "How Random Forest is different from simple bagging? \n",
    "\n",
    "In bagging, all the models are trained on the same features, while in Random Forest, each model is trained on a random subset of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10\n",
    "What is gradient boosting? Is it a good idea to use linear regression as a basic model (element of the ensemble) in gradient boosting? What about logistic regression? Why?\n",
    "\n",
    "In Gradient Boosting we train a number of weak models, usually decision trees, in such a way that each model tries to correct the mistakes of the previous model.\n",
    "\n",
    "Decision Trees are usually used as they can capture non-linear relationships which linear models can't do as well.\n",
    "\n",
    "It is not a good idea to use Linear Regression as the base model as it won't be able to accurately capture and correct the mistakes of the previous model due to its linearity.\n",
    "\n",
    "Logistic Regression although a linear model, can be used as a base model in Gradient Boosting as it can capture non-linear relationships by using polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optional discussion\n",
    "It is optional. We will contact you in private messages if needed."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
